
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"\rI am Jiarong Lin (ÊûóÂÆ∂Ëç£), graduated from The University of Hong Kong (HKU) with a Ph.D. degree in robotics. My research interests lie in the areas of Simultaneous Localization and Mapping (SLAM), Multi-Sensor Fusion, and 3D Reconstruction. I have a proven track record of producing high-quality research that is the first author of 9 paper, including 2√óT-RO, 1√óT-PAMI (in revision), 1√óRA-L journal, and 3√óICRA, 2√óIROS conference paper. (see my publication list for details). In addition to my academic pursuits, I am also an active open-source contributorüòä. I have been greatly benefited from open-source communities, and correspondingly, I have dedicated my contributions to this community as well. I have made all the code for my publications available on GitHub, where it has received over 7.8k stars‚≠ê from the community. Some of my most popular works include R3LIVE (‚òÖ1.8k), FAST-LIO (‚òÖ2.1k), loam-livox (‚òÖ1.4k), R2LIVE (‚òÖ0.7k), and ImMeshüÜï(‚òÖ0.5k).¬†I am dedicated to producing high-quality research and making meaningful contributions to both academics and industry.\n","date":1673827200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1673827200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am Jiarong Lin (ÊûóÂÆ∂Ëç£), graduated from The University of Hong Kong (HKU) with a Ph.D. degree in robotics. My research interests lie in the areas of Simultaneous Localization and Mapping (SLAM), Multi-Sensor Fusion, and 3D Reconstruction.","tags":null,"title":"Jiarong Lin","type":"authors"},{"authors":[],"categories":null,"content":"PPT Slide: Simultaneous_Localization_and_Mapping_with_Multi-sensor_Fusion.pdf\nTalks: [Online] Simultaneous Localization and Mapping with Multi-sensor Fusion (Âü∫‰∫éÂ§ö‰º†ÊÑüÂô®ËûçÂêàÁöÑÂÆö‰ΩçÂíåÂª∫ÂõæÁ≥ªÁªü)\nContents LiDAR(-inertial) SLAM World‚Äôs first LiDAR odometry and mapping (LOAM) system for solid-state LiDAR (loam-livox) Tightly-coupled LiDAR-inertial odometry (FAST-LIO) Multi-sensor Fusion (LiDAR-Inertial-Visual) World‚Äôs first open-source tightly-coupled LiDAR-Inertial-Visual system (R2LIVE) Real-time radiance map reconstruction package (R3LIVE) Immediate LiDAR localization and meshing Framework (ImMesh) Introduction to ImMesh, experiments and results. Applications based on ImMesh ImMesh for LiDAR point cloud reinforcement ImMesh for Rapid, Lossless texture reconstruction ","date":1673899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673899200,"objectID":"743fed2d7ea3884de3e840a09d593f9c","permalink":"https://ziv-lin.github.io/talk/simultaneous-localization-and-mapping-with-multi-sensor-fusion/","publishdate":"2023-01-16T23:00:00Z","relpermalink":"/talk/simultaneous-localization-and-mapping-with-multi-sensor-fusion/","section":"event","summary":"Invited by [shenlanxueyuan.com](https://www.shenlanxueyuan.com/), I give an online talk on [\"Simultaneous Localization and Mapping with Multi-sensor Fusion\"](https://www.shenlanxueyuan.com/open/course/181). In this talk, I shared my researches in my Ph.D. studies.","tags":[],"title":"Simultaneous Localization and Mapping with Multi-sensor Fusion","type":"event"},{"authors":["Jiarong Lin"],"categories":null,"content":" 1. Introduction Invited by shenlanxueyuan.com, I give an online talk on ‚ÄúSimultaneous Localization and Mapping with Multi-sensor Fusion (Âü∫‰∫éÂ§ö‰º†ÊÑüÂô®ËûçÂêàÁöÑÂÆö‰ΩçÂíåÂª∫ÂõæÁ≥ªÁªü)‚Äù. In this talk, I shared my researches in my Ph.D. studies.\nPPT Slide: Simultaneous_Localization_and_Mapping_with_Multi-sensor_Fusion.pdf\n2. Contents LiDAR(-inertial) SLAM World‚Äôs first LiDAR odometry and mapping (LOAM) system for solid-state LiDAR (loam-livox) Tightly-coupled LiDAR-inertial odometry (FAST-LIO) Multi-sensor Fusion (LiDAR-Inertial-Visual) World‚Äôs first open-source tightly-coupled LiDAR-Inertial-Visual system (R2LIVE) Real-time radiance map reconstruction package (R3LIVE) Immediate LiDAR localization and meshing Framework (ImMesh) Introduction to ImMesh, experiments and results. Applications based on ImMesh ImMesh for LiDAR point cloud reinforcement ImMesh for Rapid, Lossless texture reconstruction ","date":1673827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673827200,"objectID":"0b1083b463eb550217888327efb4331c","permalink":"https://ziv-lin.github.io/post/shenlang_talk/","publishdate":"2023-01-16T00:00:00Z","relpermalink":"/post/shenlang_talk/","section":"post","summary":"Invited by [shenlanxueyuan.com](https://www.shenlanxueyuan.com/), I give an online talk on [\"Simultaneous Localization and Mapping with Multi-sensor Fusion (Âü∫‰∫éÂ§ö‰º†ÊÑüÂô®ËûçÂêàÁöÑÂÆö‰ΩçÂíåÂª∫ÂõæÁ≥ªÁªü)\"](https://www.shenlanxueyuan.com/open/course/181). In this talk, I shared my researches in my Ph.D. studies.","tags":["Multi-sensor Fusion","LiDAR SLAM","LiDER-Inertial-Visual","3D reconstruction"],"title":"Gave a talk on \"Simultaneous Localization and Mapping with Multi-sensor Fusion\" at shenlanxueyuan.com (Audiences: 13,000+).","type":"post"},{"authors":["Jiarong Lin","Chongjiang Yuan","Yixi Cai","Haotian Li","Yuying Zou","Xiaoping Hong","Fu Zhang"],"categories":null,"content":"\rImMesh: An Immediate LiDAR Localization and Meshing Framework 1. Introduction ImMesh is a novel LiDAR(-inertial) odometry and meshing framework, which takes advantage of input of LiDAR data, achieving the goal of simultaneous localization and meshing in real-time. ImMesh comprises four tightly-coupled modules: receiver, localization, meshing, and broadcaster. The localization module utilizes the prepossessed sensor data from the receiver, estimates the sensor pose online by registering LiDAR scans to maps, and dynamically grows the map. Then, our meshing module takes the registered LiDAR scan for incrementally reconstructing the triangle mesh on the fly. Finally, the real-time odometry, map, and mesh are published via our broadcaster.\n1.2 Our accompanying videos Our accompanying videos are now available on YouTube (click below images to open) and Bilibili1, 2, 3.\n2. What can ImMesh do? 2.1 Simultaneous LiDAR localization and mesh reconstruction on the fly 2.2 ImMesh for LiDAR point cloud reinforement 2.3 ImMesh for rapid, lossless texture reconstruction 3. Contact us If you have any questions about this work, please feel free to contact me \u0026lt;ziv.lin.ljrATgmail.com\u0026gt; and Dr. Fu Zhang \u0026lt;fuzhangAThku.hk\u0026gt; via email.\n","date":1673568000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673568000,"objectID":"e81a8ce0338001b75eac7b059209ce83","permalink":"https://ziv-lin.github.io/publication/paper_immesh/","publishdate":"2023-01-13T00:00:00Z","relpermalink":"/publication/paper_immesh/","section":"publication","summary":"In this paper, we propose a novel LiDAR(-inertial) odometry and mapping framework to achieve the goal of simultaneous localization and meshing in real-time. This proposed framework termed ImMesh comprises four tightly-coupled modules: receiver, localization, meshing, and broadcaster. The localization module utilizes the prepossessed sensor data from the receiver, estimates the sensor pose online by registering LiDAR scans to maps, and dynamically grows the map. Then, our meshing module takes the registered LiDAR scan for incrementally reconstructing the triangle mesh on the fly. Finally, the real-time odometry, map, and mesh are published via our broadcaster. The key contribution of this work is the meshing module, which represents a scene by an efficient hierarchical voxels structure, performs fast finding of voxels observed by new scans, and reconstructs triangle facets in each voxel in an incremental manner. This voxel-wise meshing operation is delicately designed for the purpose of efficiency; it first performs a dimension reduction by projecting 3D points to a 2D local plane contained in the voxel, and then executes the meshing operation with pull, commit and push steps for incremental reconstruction of triangle facets. To the best of our knowledge, this is the first work in literature that can reconstruct online the triangle mesh of large-scale scenes, just relying on a standard CPU without GPU acceleration. To share our findings and make contributions to the community, we make our code publicly available on our GitHub: https://github.com/hku-mars/ImMesh","tags":["ImMesh","LiDAR SLAM","Multi-sensor Fusion","LiDER-Inertial","3D reconstruction"],"title":"ImMesh: An Immediate LiDAR Localization and Meshing Framework","type":"publication"},{"authors":["Jiarong Lin","Chongjiang Yuan","Yixi Cai","Haotian Li","Yuying Zou","Xiaoping Hong","Fu Zhang"],"categories":null,"content":"\rImMesh: An Immediate LiDAR Localization and Meshing Framework 1. Introduction ImMesh is a novel LiDAR(-inertial) odometry and meshing framework, which takes advantage of input of LiDAR data, achieving the goal of simultaneous localization and meshing in real-time. ImMesh comprises four tightly-coupled modules: receiver, localization, meshing, and broadcaster. The localization module utilizes the prepossessed sensor data from the receiver, estimates the sensor pose online by registering LiDAR scans to maps, and dynamically grows the map. Then, our meshing module takes the registered LiDAR scan for incrementally reconstructing the triangle mesh on the fly. Finally, the real-time odometry, map, and mesh are published via our broadcaster.\n1.2 Our accompanying videos Our accompanying videos are now available on YouTube (click below images to open) and Bilibili1, 2, 3.\n2. What can ImMesh do? 2.1 Simultaneous LiDAR localization and mesh reconstruction on the fly 2.2 ImMesh for LiDAR point cloud reinforement 2.3 ImMesh for rapid, lossless texture reconstruction 3. Contact us If you have any questions about this work, please feel free to contact me \u0026lt;ziv.lin.ljrATgmail.com\u0026gt; and Dr. Fu Zhang \u0026lt;fuzhangAThku.hk\u0026gt; via email.\n","date":1673568000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673568000,"objectID":"2d39a6738d4ec4efce5b5d7d1084d35f","permalink":"https://ziv-lin.github.io/project/proj_immesh/","publishdate":"2023-01-13T00:00:00Z","relpermalink":"/project/proj_immesh/","section":"project","summary":"**ImMesh** is a novel LiDAR(-inertial) odometry and meshing framework, which takes advantage of input of LiDAR data, achieving the goal of **simultaneous localization and meshing** in real-time. ImMesh comprises four tightly-coupled modules: *receiver*, *localization*, *meshing*, and *broadcaster*. The *localization* module utilizes the prepossessed sensor data from the *receiver*, estimates the sensor pose online by registering LiDAR scans to maps, and dynamically grows the map. Then, our *meshing* module takes the registered LiDAR scan for **incrementally reconstructing the triangle mesh on the fly**. Finally, the real-time odometry, map, and mesh are published via our *broadcaster*.","tags":["ImMesh","LiDAR SLAM","Multi-sensor Fusion","LiDER-Inertial","3D reconstruction"],"title":"üÜïImMesh: An Immediate LiDAR Localization and Meshing Framework","type":"project"},{"authors":["Jiarong Lin","Chongjiang Yuan","Yixi Cai","Haotian Li","Yuying Zou","Xiaoping Hong","Fu Zhang"],"categories":null,"content":"1. Introduction ImMesh is a novel LiDAR(-inertial) odometry and meshing framework, which takes advantage of input of LiDAR data, achieving the goal of simultaneous localization and meshing in real-time. ImMesh comprises four tightly-coupled modules: receiver, localization, meshing, and broadcaster. The localization module utilizes the prepossessed sensor data from the receiver, estimates the sensor pose online by registering LiDAR scans to maps, and dynamically grows the map. Then, our meshing module takes the registered LiDAR scan for incrementally reconstructing the triangle mesh on the fly. Finally, the real-time odometry, map, and mesh are published via our broadcaster.\n1.2 Our accompanying videos Our accompanying videos are now available on YouTube (click below images to open) and Bilibili1, 2, 3.\n2. What can ImMesh do? 2.1 Simultaneous LiDAR localization and mesh reconstruction on the fly 2.2 ImMesh for LiDAR point cloud reinforement 2.3 ImMesh for rapid, lossless texture reconstruction 3. Contact us If you have any questions about this work, please feel free to contact me \u0026lt;ziv.lin.ljrATgmail.com\u0026gt; and Dr. Fu Zhang \u0026lt;fuzhangAThku.hk\u0026gt; via email.\n","date":1673568000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673568000,"objectID":"024e33930e44b9319abb9f7e22aa1e0a","permalink":"https://ziv-lin.github.io/post/release_of_immesh/","publishdate":"2023-01-13T00:00:00Z","relpermalink":"/post/release_of_immesh/","section":"post","summary":"**ImMesh** is a novel LiDAR(-inertial) odometry and meshing framework, which takes advantage of input of LiDAR data, achieving the goal of **simultaneous localization and meshing** in real-time.","tags":["Multi-sensor Fusion","LiDAR SLAM","LiDER-Inertial-Visual","3D reconstruction"],"title":"Release of work \"ImMesh: An Immediate LiDAR Localization and Meshing Framework\"","type":"post"},{"authors":["Chongjiang Yuan","Jiarong Lin","Zuhao Zou","Xiaoping Hong","Fu Zhang"],"categories":null,"content":"Introduction STD is a global descriptor for 3D place recognition. For a triangle, its shape is uniquely determined by the length of the sides or included angles. Moreover, the shape of triangles is completely invariant to rigid transformations. Based on this property, we first design an algorithm to efficiently extract local key points from the 3D point cloud and encode these key points into triangular descriptors. Then, place recognition is achieved by matching the side lengths (and some other information) of the descriptors between point clouds. The point correspondence obtained from the descriptor matching pair can be further used in geometric verification, which greatly improves the accuracy of place recognition.\nA typical place recognition case with STD. These two frames of point clouds are collected by a small FOV LiDAR (Livox Avia) moving in opposite directions, resulting in a low point cloud overlap and drastic viewpoint change.\rRelated video Our accompanying video is now available on YouTube.\n","date":1664150400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664150400,"objectID":"938dbb675254e6dd200de86134b82beb","permalink":"https://ziv-lin.github.io/publication/paper_std/","publishdate":"2022-09-26T00:00:00Z","relpermalink":"/publication/paper_std/","section":"publication","summary":"In this work, we present a novel global descriptor termed *stable triangle descriptor (STD)* for 3D place recognition. For a triangle, its shape is uniquely determined by the length of the sides or included angles. Moreover, the shape of triangles is completely invariant to rigid transformations. Based on this property, we first design an algorithm to efficiently extract local key points from the 3D point cloud and encode these key points into triangular descriptors. Then, place recognition is achieved by matching the side lengths (and some other information) of the descriptors between point clouds. The point correspondence obtained from the descriptor matching pair can be further used in geometric verification, which greatly improves the accuracy of place recognition. In our experiments, we extensively compare our proposed system against other state-of-the-art systems (i.e., M2DP, Scan Context) on public datasets (i.e., KITTI, NCLT, and Complex-Urban) and our self-collected dataset (with a non-repetitive scanning solid-state LiDAR). All the quantitative results show that STD has stronger adaptability and a great improvement in precision over its counterparts. To share our findings and make contributions to the community, we open source our code on our GitHub: https://github.com/hku-mars/STD.","tags":["LiDAR SLAM","Loop detection","Loop closure"],"title":"STD: Stable Triangle Descriptor for 3D place recognition","type":"publication"},{"authors":["Xinyi Chen","Boyu Zhou","Jiarong Lin","Yichen Zhang","Fu Zhang","Shaojie Shen"],"categories":null,"content":"","date":1659916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659916800,"objectID":"7cf7d44bda02dc1e00517d94fa333885","permalink":"https://ziv-lin.github.io/publication/paper_skeleton/","publishdate":"2022-08-08T00:00:00Z","relpermalink":"/publication/paper_skeleton/","section":"publication","summary":"In recent years, mobile robots are becoming ambitious and deployed in large-scale scenarios. Serving as a high-level understanding of environments, a sparse skeleton graph is beneficial for more efficient global planning. Currently, existing solutions for skeleton graph generation suffer from several major limitations, including poor adaptiveness to different map representations, dependency on robot inspection trajectories and high computational overhead. In this paper, we propose an efficient and flexible algorithm generating a trajectory-independent 3D sparse topological skeleton graph capturing the spatial structure of the free space. In our method, an efficient ray sampling and validating mechanism are adopted to find distinctive free space regions, which contributes to skeleton graph vertices, with traversability between adjacent vertices as edges. A cycle formation scheme is also utilized to maintain skeleton graph compactness. Benchmark comparison with state-of-the-art works demonstrates that our approach generates sparse graphs in a substantially shorter time, giving high-quality global planning paths. Experiments conducted in real-world maps further validate the capability of our method in real-world scenarios. Our method will be made open source to benefit the community.","tags":["Motion Planning"],"title":"Fast 3D Sparse Topological Skeleton Graph Generation for Mobile Robot Global Planning","type":"publication"},{"authors":["Fanze Kong","Xiyuan Liu","Benxu Tang","Jiarong Lin","Yunfan Ren","Yixi Cai","Fangcheng Zhu","Nan Chen","Fu Zhang"],"categories":null,"content":"","date":1659916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659916800,"objectID":"6b9a9af85d2c3d4b2828d83e827e414c","permalink":"https://ziv-lin.github.io/publication/paper_marsim/","publishdate":"2022-08-08T00:00:00Z","relpermalink":"/publication/paper_marsim/","section":"publication","summary":"The emergence of low-cost, small form factor and light-weight solid-state LiDAR sensors have brought new opportunities for autonomous unmanned aerial vehicles (UAVs) by advancing navigation safety and computation efficiency. Yet the successful developments of LiDAR-based UAVs must rely on extensive simulations. Existing simulators can hardly perform simulations of real-world environments due to the requirements of dense mesh maps that are difficult to obtain. In this paper, we develop a point-realistic simulator of real-world scenes for LiDAR-based UAVs. The key idea is the underlying point rendering method, where we construct a depth image directly from the point cloud map and interpolate it to obtain realistic LiDAR point measurements. Our developed simulator is able to run on a light-weight computing platform and supports the simulation of LiDARs with different resolution and scanning patterns, dynamic obstacles, and multi-UAV systems. Developed in the ROS framework, the simulator can easily communicate with other key modules of an autonomous robot, such as perception, state estimation, planning, and control. Finally, the simulator provides 10 high-resolution point cloud maps of various real-world environments, including forests of different densities, historic building, office, parking garage, and various complex indoor environments. These realistic maps provide diverse testing scenarios for an autonomous UAV. Evaluation results show that the developed simulator achieves superior performance in terms of time and memory consumption against Gazebo and that the simulated UAV flights highly match the actual one in real-world environments. We believe such a point-realistic and light-weight simulator is crucial to bridge the gap between UAV simulation and experiments and will significantly facilitate the research of LiDAR-based autonomous UAVs in the future.","tags":["Motion Planning"],"title":"MARSIM: A light-weight point-realistic simulator for LiDAR-based UAVs","type":"publication"},{"authors":["Jiarong Lin","Fu Zhang"],"categories":null,"content":"1. Introduction R3LIVE is a novel LiDAR-Inertial-Visual sensor fusion framework, which takes advantage of measurement of LiDAR, inertial, and visual sensors to achieve robust and accurate state estimation. R3LIVE is built upon our previous work R2LIVE, is contained of two subsystems: the LiDAR-inertial odometry (LIO) and the visual-inertial odometry (VIO). The LIO subsystem (FAST-LIO) takes advantage of the measurement from LiDAR and inertial sensors and builds the geometric structure of (i.e. the position of 3D points) global maps. The VIO subsystem utilizes the data of visual-inertial sensors and renders the map‚Äôs texture (i.e. the color of 3D points). 1.1 Our accompanying videos Our accompanying videos are now available on YouTube (click below images to open) and Bilibili1, 2.\n1.2 Our associate dataset: R3LIVE-dataset Our associate dataset R3LIVE-dataset that use for evaluation is also available online. You can access and download our datasets via this Github repository.\n1.3 Our open-source hardware design All of the mechanical modules of our handheld device that use for data collection are designed as FDM printable, with the schematics of the design are also open-sourced in this Github repository.\n2. R3LIVE Features 2.1 Strong robustness in various challenging scenarios R3LIVE is robust enough to work well in various of LiDAR-degenerated scenarios (see following figures):\nAnd even in simultaneously LiDAR degenerated and visual texture-less environments (see Experiment-1 of our paper).\n2.2 Real-time RGB maps reconstruction R3LIVE is able to reconstruct the precise, dense, 3D, RGB-colored maps of surrounding environment in real-time (watch this video).\n2.3 Ready for 3D applications To make R3LIVE more extensible, we also provide a series of offline utilities for reconstructing and texturing meshes, which further reduce the gap between R3LIVE and various 3D applications (watch this video).\n3. Acknowledgments In the development of R3LIVE, we stand on the shoulders of the following repositories:\nR2LIVE: A robust, real-time tightly-coupled multi-sensor fusion package. FAST-LIO: A computationally efficient and robust LiDAR-inertial odometry package. ikd-Tree: A state-of-art dynamic KD-Tree for 3D kNN search. livox_camera_calib: A robust, high accuracy extrinsic calibration tool between high resolution LiDAR (e.g. Livox) and camera in targetless environment. LOAM-Livox: A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR. openMVS: A library for computer-vision scientists and especially targeted to the Multi-View Stereo reconstruction community. VCGlib: An open source, portable, header-only Visualization and Computer Graphics Library. CGAL: A C++ Computational Geometry Algorithms Library. ","date":1659744000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659744000,"objectID":"9847a987403af1157b776bfe003baec1","permalink":"https://ziv-lin.github.io/publication/paper_r3live_pp/","publishdate":"2022-08-06T00:00:00Z","relpermalink":"/publication/paper_r3live_pp/","section":"publication","summary":"This work proposed a LiDAR-inertial-visual fusion framework termed R$^3$LIVE++ to achieve robust and accurate state estimation while simultaneously reconstructing the radiance map on the fly. R$^3$LIVE++ consists of a LiDAR-inertial odometry (LIO) and a visual-inertial odometry (VIO), both running in real-time. The LIO subsystem utilizes the measurements from a LiDAR for reconstructing the geometric structure, while the VIO subsystem simultaneously recovers the radiance information of the geometric structure from the input images. R$^3$LIVE++ is developed based on R$^3$LIVE and further improves the accuracy in localization and mapping by accounting for the camera photometric calibration and the online estimation of camera exposure time. We conduct more extensive experiments on public and private datasets to compare our proposed system against other state-of-the-art SLAM systems. Quantitative and qualitative results show that R$^3$LIVE++ has significant improvements over others in both accuracy and robustness. Moreover, to demonstrate the extendability of R$^3$LIVE++, we developed several applications based on our reconstructed maps, such as high dynamic range (HDR) imaging, virtual environment exploration, and 3D video gaming. Lastly, to share our findings and make contributions to the community, we release our codes, hardware design, and dataset on our Github: https://github.com/hku-mars/r3live","tags":["R3LIVE","LiDAR SLAM","Multi-sensor Fusion","LiDER-Inertial-Visual","3D reconstruction"],"title":"R$^3$LIVE++: A Robust, Real-time, Radiance reconstruction package with a tightly-coupled LiDAR-Inertial-Visual state Estimator","type":"publication"},{"authors":["Jiarong Lin","Fu Zhang"],"categories":null,"content":"\r1. Introduction R3LIVE is a novel LiDAR-Inertial-Visual sensor fusion framework, which takes advantage of measurement of LiDAR, inertial, and visual sensors to achieve robust and accurate state estimation. R3LIVE is built upon our previous work R2LIVE, is contained of two subsystems: the LiDAR-inertial odometry (LIO) and the visual-inertial odometry (VIO). The LIO subsystem (FAST-LIO) takes advantage of the measurement from LiDAR and inertial sensors and builds the geometric structure of (i.e. the position of 3D points) global maps. The VIO subsystem utilizes the data of visual-inertial sensors and renders the map‚Äôs texture (i.e. the color of 3D points).\n1.1 Our accompanying videos Our accompanying videos are now available on YouTube (click below images to open) and Bilibili1, 2.\n1.2 Our associate dataset: R3LIVE-dataset Our associate dataset R3LIVE-dataset that use for evaluation is also available online. You can access and download our datasets via this Github repository.\n1.3 Our open-source hardware design All of the mechanical modules of our handheld device that use for data collection are designed as FDM printable, with the schematics of the design are also open-sourced in this Github repository.\n2. R3LIVE Features 2.1 Strong robustness in various challenging scenarios R3LIVE is robust enough to work well in various of LiDAR-degenerated scenarios (see following figures):\nAnd even in simultaneously LiDAR degenerated and visual texture-less environments (see Experiment-1 of our paper).\n2.2 Real-time RGB maps reconstruction R3LIVE is able to reconstruct the precise, dense, 3D, RGB-colored maps of surrounding environment in real-time (watch this video).\n2.3 Ready for 3D applications To make R3LIVE more extensible, we also provide a series of offline utilities for reconstructing and texturing meshes, which further reduce the gap between R3LIVE and various 3D applications (watch this video).\n3. Acknowledgments In the development of R3LIVE, we stand on the shoulders of the following repositories:\nR2LIVE: A robust, real-time tightly-coupled multi-sensor fusion package. FAST-LIO: A computationally efficient and robust LiDAR-inertial odometry package. ikd-Tree: A state-of-art dynamic KD-Tree for 3D kNN search. livox_camera_calib: A robust, high accuracy extrinsic calibration tool between high resolution LiDAR (e.g. Livox) and camera in targetless environment. LOAM-Livox: A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR. openMVS: A library for computer-vision scientists and especially targeted to the Multi-View Stereo reconstruction community. VCGlib: An open source, portable, header-only Visualization and Computer Graphics Library. CGAL: A C++ Computational Geometry Algorithms Library. ","date":1657584000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657584000,"objectID":"df29f6089672af1119c5eedb246f4947","permalink":"https://ziv-lin.github.io/project/proj_r3live/","publishdate":"2022-07-12T00:00:00Z","relpermalink":"/project/proj_r3live/","section":"project","summary":"**R$^3$LIVE** is a versatile and well-engineered system toward various possible applications, which can not only serve as a SLAM system for realtime robotic applications but can also reconstruct the dense, precise, RGB-colored 3D maps for applications like surveying and mapping. In addition, we have developed a series of offline utilities for reconstructing and texturing meshes for various of 3D applications.","tags":["R3LIVE","LiDAR SLAM","Multi-sensor Fusion","LiDER-Inertial-Visual","3D reconstruction"],"title":"R$^3$LIVE: A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package","type":"project"},{"authors":["Jiarong Lin","Fu Zhang"],"categories":null,"content":"\r1. Introduction R3LIVE is a novel LiDAR-Inertial-Visual sensor fusion framework, which takes advantage of measurement of LiDAR, inertial, and visual sensors to achieve robust and accurate state estimation. R3LIVE is built upon our previous work R2LIVE, is contained of two subsystems: the LiDAR-inertial odometry (LIO) and the visual-inertial odometry (VIO). The LIO subsystem (FAST-LIO) takes advantage of the measurement from LiDAR and inertial sensors and builds the geometric structure of (i.e. the position of 3D points) global maps. The VIO subsystem utilizes the data of visual-inertial sensors and renders the map‚Äôs texture (i.e. the color of 3D points). 1.1 Our accompanying videos Our accompanying videos are now available on YouTube (click below images to open) and Bilibili1, 2.\n1.2 Our associate dataset: R3LIVE-dataset Our associate dataset R3LIVE-dataset that use for evaluation is also available online. You can access and download our datasets via this Github repository.\n1.3 Our open-source hardware design All of the mechanical modules of our handheld device that use for data collection are designed as FDM printable, with the schematics of the design are also open-sourced in this Github repository.\n2. R3LIVE Features 2.1 Strong robustness in various challenging scenarios R3LIVE is robust enough to work well in various of LiDAR-degenerated scenarios (see following figures):\nAnd even in simultaneously LiDAR degenerated and visual texture-less environments (see Experiment-1 of our paper).\n2.2 Real-time RGB maps reconstruction R3LIVE is able to reconstruct the precise, dense, 3D, RGB-colored maps of surrounding environment in real-time (watch this video).\n2.3 Ready for 3D applications To make R3LIVE more extensible, we also provide a series of offline utilities for reconstructing and texturing meshes, which further reduce the gap between R3LIVE and various 3D applications (watch this video).\n3. Acknowledgments In the development of R3LIVE, we stand on the shoulders of the following repositories:\nR2LIVE: A robust, real-time tightly-coupled multi-sensor fusion package. FAST-LIO: A computationally efficient and robust LiDAR-inertial odometry package. ikd-Tree: A state-of-art dynamic KD-Tree for 3D kNN search. livox_camera_calib: A robust, high accuracy extrinsic calibration tool between high resolution LiDAR (e.g. Livox) and camera in targetless environment. LOAM-Livox: A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR. openMVS: A library for computer-vision scientists and especially targeted to the Multi-View Stereo reconstruction community. VCGlib: An open source, portable, header-only Visualization and Computer Graphics Library. CGAL: A C++ Computational Geometry Algorithms Library. ","date":1657584000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657584000,"objectID":"1ce813e9ea197ac119691478754ec289","permalink":"https://ziv-lin.github.io/publication/paper_r3live/","publishdate":"2022-07-12T00:00:00Z","relpermalink":"/publication/paper_r3live/","section":"publication","summary":"In this paper, we propose a novel LiDAR-Inertial-Visual sensor fusion framework termed R3LIVE, which takes advantage of measurement of LiDAR, inertial, and visual sensors to achieve robust and accurate state estimation. R$^3$LIVE consists of two subsystems, a LiDAR-Inertial odometry (LIO) and a Visual-Inertial odometry (VIO). The LIO subsystem (FAST-LIO) utilizes the measurements from LiDAR and inertial sensors and builds the geometric structure (i.e., the positions of 3D points) of the map. The VIO subsystem uses the data of Visual-Inertial sensors and renders the map's texture (i.e., the color of 3D points). More specifically, the VIO subsystem fuses the visual data directly and effectively by minimizing the frame-to-map photometric error. The proposed system R3LIVE is developed based on our previous work R$^2$LIVE, with a completely different VIO architecture design. The overall system is able to reconstruct the precise, dense, 3D, RGB-colored maps of the surrounding environment in real-time (see our attached video https://youtu.be/j5fT8NE5fdg). Our experiments show that the resultant system achieves higher robustness and accuracy in state estimation than its current counterparts. To share our findings and make contributions to the community, we open source R$^3$LIVE on our Github: https://github.com/hku-mars/r3live.","tags":["R3LIVE","LiDAR SLAM","Multi-sensor Fusion","LiDER-Inertial-Visual","3D reconstruction"],"title":"R$^3$LIVE: A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package","type":"publication"},{"authors":["Wei Xu","Yixi Cai","Dongjiao He","Jiarong Lin","Fu Zhang"],"categories":null,"content":"FAST-LIO FAST-LIO (Fast LiDAR-Inertial Odometry) is a computationally efficient and robust LiDAR-inertial odometry package. It fuses LiDAR feature points with IMU data using a tightly-coupled iterated extended Kalman filter to allow robust navigation in fast-motion, noisy or cluttered environments where degeneration occurs. Our package address many key issues:\nFast iterated Kalman filter for odometry optimization; Automaticaly initialized at most steady environments; Parallel KD-Tree Search to decrease the computation; FAST-LIO 2.0 (2021-07-05 Update) Related video: FAST-LIO2, FAST-LIO1\nFAST-LIO 2.0 Features:\nIncremental mapping using ikd-Tree, achieve faster speed and over 100Hz LiDAR rate. Direct odometry (scan to map) on Raw LiDAR points (feature extraction can be disabled), achieving better accuracy. Since no requirements for feature extraction, FAST-LIO2 can support many types of LiDAR including spinning (Velodyne, Ouster) and solid-state (Livox Avia, Horizon, MID-70) LiDARs, and can be easily extended to support more LiDARs. Support external IMU. Support ARM-based platforms including Khadas VIM3, Nivida TX2, Raspberry Pi 4B(8G RAM). ","date":1626307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626307200,"objectID":"c5797953a7870ce4ee1ef336aef1a59b","permalink":"https://ziv-lin.github.io/project/proj_fastlio/","publishdate":"2022-07-15T00:00:00Z","relpermalink":"/project/proj_fastlio/","section":"project","summary":"**FAST-LIO** (Fast LiDAR-Inertial Odometry) is a computationally efficient and robust LiDAR-inertial odometry package. It fuses LiDAR feature points with IMU data using a tightly-coupled iterated extended Kalman filter to allow robust navigation in fast-motion, noisy or cluttered environments where degeneration occurs. Our package addresses many key issues: 1) Fast error state iterated Kalman filter (ESIKF) for odometry optimization. 2) Incremental mapping using ikd-Tree, achieve faster speed and over 100Hz LiDAR rate. 3) Without the need for feature extraction, FAST-LIO2 can support many types of LiDAR including spinning (Velodyne, Ouster) and solid-state (Livox Avia, Horizon, MID-70) LiDARs, and can be easily extended to support more LiDARs.","tags":["FAST-LIO","LiDAR SLAM","Multi-sensor Fusion","LiDER-Inertial","3D reconstruction"],"title":"FAST-LIO2: Fast Direct LiDAR-inertial Odometry","type":"project"},{"authors":["Wei Xu","Yixi Cai","Dongjiao He","Jiarong Lin","Fu Zhang"],"categories":null,"content":"FAST-LIO FAST-LIO (Fast LiDAR-Inertial Odometry) is a computationally efficient and robust LiDAR-inertial odometry package. It fuses LiDAR feature points with IMU data using a tightly-coupled iterated extended Kalman filter to allow robust navigation in fast-motion, noisy or cluttered environments where degeneration occurs. Our package address many key issues:\nFast iterated Kalman filter for odometry optimization; Automaticaly initialized at most steady environments; Parallel KD-Tree Search to decrease the computation; FAST-LIO 2.0 (2021-07-05 Update) Related video: FAST-LIO2, FAST-LIO1\nFAST-LIO 2.0 Features:\nIncremental mapping using ikd-Tree, achieve faster speed and over 100Hz LiDAR rate. Direct odometry (scan to map) on Raw LiDAR points (feature extraction can be disabled), achieving better accuracy. Since no requirements for feature extraction, FAST-LIO2 can support many types of LiDAR including spinning (Velodyne, Ouster) and solid-state (Livox Avia, Horizon, MID-70) LiDARs, and can be easily extended to support more LiDARs. Support external IMU. Support ARM-based platforms including Khadas VIM3, Nivida TX2, Raspberry Pi 4B(8G RAM). ","date":1626307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626307200,"objectID":"088dfc0f3de88f5f7096fe3d60bcd49f","permalink":"https://ziv-lin.github.io/publication/paper_fast_lio2/","publishdate":"2022-07-15T00:00:00Z","relpermalink":"/publication/paper_fast_lio2/","section":"publication","summary":"This paper presents FAST-LIO2: a fast, robust, and versatile LiDAR-inertial odometry framework. Building on a highly efficient tightly-coupled iterated Kalman filter, FAST-LIO2 has two key novelties that allow fast, robust, and accurate LiDAR navigation (and mapping). The first one is directly registering raw points to the map (and subsequently update the map, i.e., mapping) without extracting features. This enables the exploitation of subtle features in the environment and hence increases the accuracy. The elimination of a hand-engineered feature extraction module also makes it naturally adaptable to emerging LiDARs of different scanning patterns; The second main novelty is maintaining a map by an incremental k-d tree data structure, ikd-Tree, that enables incremental updates (i.e., point insertion, delete) and dynamic re-balancing. Compared with existing dynamic data structures (octree, R$^*$-tree, nanoflann k-d tree), ikd-Tree achieves superior overall performance while naturally supports downsampling on the tree. We conduct an exhaustive benchmark comparison in 19 sequences from a variety of open LiDAR datasets. FAST-LIO2 achieves consistently higher accuracy at a much lower computation load than other state-of-the-art LiDAR-inertial navigation systems. Various real-world experiments on solid-state LiDARs with small FoV are also conducted. Overall, FAST-LIO2 is computationally-efficient (e.g., up to 100 $Hz$ odometry and mapping in large outdoor environments), robust (e.g., reliable pose estimation in cluttered indoor environments with rotation up to 1000 $deg/s$), versatile (i.e., applicable to both multi-line spinning and solid-state LiDARs, UAV and handheld platforms, and Intel and ARM-based processors), while still achieving higher accuracy than existing methods. Our implementation of the system FAST-LIO2, and the data structure ikd-Tree are both open-sourced on Github.","tags":["FAST-LIO","LiDAR SLAM","Multi-sensor Fusion","LiDER-Inertial","3D reconstruction"],"title":"FAST-LIO2: Fast Direct LiDAR-inertial Odometry","type":"publication"},{"authors":["Jiarong Lin","Fu Zhang"],"categories":null,"content":"\rIntroduction R2LIVE is a robust, real-time tightly-coupled multi-sensor fusion framework, which fuses the measurement from the LiDAR, inertial sensor, visual camera to achieve robust, accurate state estimation. Taking advantage of measurement from all individual sensors, our algorithm is robust enough to various visual failure, LiDAR-degenerated scenarios, and is able to run in real time on an on-board computation platform, as shown by extensive experiments conducted in indoor, outdoor, and mixed environment of different scale.\nThe reconstructed 3D maps of HKU main building are shown in (d), and the detail point cloud with the correspondence panorama images are shown in (a) and (b). (c) shows that our algorithm can close the loop by itself (returning the starting point) without any additional processing (e.g. loop closure). In (e), we merge our map with the satellite image to further examine the accuracy of our system.\rOur related video: our related video is now available on YouTube (click below images to open):\n","date":1625875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625875200,"objectID":"45598c996963d5142424ecd7789179ca","permalink":"https://ziv-lin.github.io/project/proj_r2live/","publishdate":"2022-07-12T00:00:00Z","relpermalink":"/project/proj_r2live/","section":"project","summary":"**R$^2$LIVE** is a robust, real-time tightly-coupled multi-sensor fusion framework, which fuses the measurement from the LiDAR, inertial sensor, visual camera to achieve robust, accurate state estimation. Taking advantage of measurement from all individual sensors, our algorithm is robust enough to various visual failure, LiDAR-degenerated scenarios, and is able to run in real time on an on-board computation platform, as shown by extensive experiments conducted in indoor, outdoor, and mixed environment of different scale.","tags":["R2LIVE","LiDAR SLAM","Multi-sensor Fusion","LiDER-Inertial-Visual","3D reconstruction"],"title":"R$^2$LIVE: A Robust, Real-time, LiDAR-Inertial-Visual tightly-coupled state Estimator and mapping","type":"project"},{"authors":["Jiarong Lin","Chunran Zheng","Wei Xu","Fu Zhang"],"categories":null,"content":"\rIntroduction R2LIVE is a robust, real-time tightly-coupled multi-sensor fusion framework, which fuses the measurement from the LiDAR, inertial sensor, visual camera to achieve robust, accurate state estimation. Taking advantage of measurement from all individual sensors, our algorithm is robust enough to various visual failure, LiDAR-degenerated scenarios, and is able to run in real time on an on-board computation platform, as shown by extensive experiments conducted in indoor, outdoor, and mixed environment of different scale.\nThe reconstructed 3D maps of HKU main building are shown in (d), and the detail point cloud with the correspondence panorama images are shown in (a) and (b). (c) shows that our algorithm can close the loop by itself (returning the starting point) without any additional processing (e.g. loop closure). In (e), we merge our map with the satellite image to further examine the accuracy of our system.\rOur related video: our related video is now available on YouTube (click below images to open):\n","date":1625875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625875200,"objectID":"0c07bdc13a185b92458f6cc9804df9f8","permalink":"https://ziv-lin.github.io/publication/paper_r2live/","publishdate":"2022-07-12T00:00:00Z","relpermalink":"/publication/paper_r2live/","section":"publication","summary":"In this letter, we propose a robust, real-time tightly-coupled multi-sensor fusion framework, which fuses measurements from LiDAR, inertial sensor, and visual camera to achieve robust and accurate state estimation. Our proposed framework is composed of two parts: the filter-based odometry and factor graph optimization. To guarantee real-time performance, we estimate the state within the framework of error-state iterated Kalman-filter, and further improve the overall precision with our factor graph optimization. Taking advantage of measurements from all individual sensors, our algorithm is robust enough to various visual failure, LiDAR-degenerated scenarios, and is able to run in real time on an on-board computation platform, as shown by extensive experiments conducted in indoor, outdoor, and mixed environments of different scale (see attached video https://youtu.be/9lqRHmlN_MA). Moreover, the results show that our proposed framework can improve the accuracy of state-of-the-art LiDAR-inertial or visual-inertial odometry. To share our findings and to make contributions to the community, we open source our codes on our Github: https://github.com/hku-mars/r2live.","tags":["R2LIVE","LiDAR SLAM","Multi-sensor Fusion","LiDER-Inertial-Visual","3D reconstruction"],"title":"R$^2$LIVE: A Robust, Real-time, LiDAR-Inertial-Visual tightly-coupled state Estimator and mapping","type":"publication"},{"authors":["Jiarong Lin","Xiyuan Liu","Fu Zhang"],"categories":null,"content":"","date":1612915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612915200,"objectID":"edcde9c60b52f6a29a89179138652ea7","permalink":"https://ziv-lin.github.io/publication/paper_dc_loam/","publishdate":"2021-02-10T00:00:00Z","relpermalink":"/publication/paper_dc_loam/","section":"publication","summary":"TODO","tags":["LiDAR SLAM","Calibration","Multi-sensor Fusion","3D reconstruction"],"title":"A decentralized framework for simultaneous calibration, localization and mapping with multiple LiDARs","type":"publication"},{"authors":["Jiarong Lin","Fu Zhang"],"categories":null,"content":"1. Introduction Loam-Livox is a robust, low drift, and real time odometry and mapping package for Livox LiDARs, significant low cost and high performance LiDARs that are designed for massive industrials uses. Our package address many key issues: feature extraction and selection in a very limited FOV, robust outliers rejection, moving objects filtering, and motion distortion compensation. In addition, we also integrate other features like parallelable pipeline, point cloud management using cells and maps, loop closure, utilities for maps saving and reload, etc. To know more about the details, please refer to our related paper:) Our related paper: our related papers are now available on arxiv:\nLoam_livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV A fast, complete, point cloud based loop closure for LiDAR odometry and mapping Our related video: our related videos are now available on YouTube (click below images to open):\nOur mapping results reconstructed with Livox-mid40 LiDAR:\n","date":1594512000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594512000,"objectID":"e098947040301b2ff3bf4b061e24efd4","permalink":"https://ziv-lin.github.io/publication/paper_loam_livox/","publishdate":"2020-07-12T00:00:00Z","relpermalink":"/publication/paper_loam_livox/","section":"publication","summary":"TODO","tags":["loam_livox","LiDAR SLAM","3D reconstruction"],"title":"Loam_livox: A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR","type":"publication"},{"authors":["Jiarong Lin","Fu Zhang"],"categories":null,"content":"\r1. Introduction Loam-Livox is a robust, low drift, and real time odometry and mapping package for Livox LiDARs, significant low cost and high performance LiDARs that are designed for massive industrials uses. Our package address many key issues: feature extraction and selection in a very limited FOV, robust outliers rejection, moving objects filtering, and motion distortion compensation. In addition, we also integrate other features like parallelable pipeline, point cloud management using cells and maps, loop closure, utilities for maps saving and reload, etc. To know more about the details, please refer to our related paper:) Our related paper: our related papers are now available on arxiv:\nLoam_livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV A fast, complete, point cloud based loop closure for LiDAR odometry and mapping Our related video: our related videos are now available on YouTube (click below images to open):\nOur mapping results reconstructed with Livox-mid40 LiDAR:\n","date":1594512000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594512000,"objectID":"effd7b41ea8f54e56e484dcab44827dc","permalink":"https://ziv-lin.github.io/project/proj_loam_livox/","publishdate":"2020-07-12T00:00:00Z","relpermalink":"/project/proj_loam_livox/","section":"project","summary":"**Loam-Livox** is a robust, low drift, and real time odometry and mapping package for [*Livox LiDARs*](https://www.livoxtech.com/), significant low cost and high performance LiDARs that are designed for massive industrials uses. Our package address many key issues: feature extraction and selection in a very limited FOV, robust outliers rejection, moving objects filtering, and motion distortion compensation. In addition, we also integrate other features like parallelable pipeline, point cloud management using cells and maps, loop closure, utilities for maps saving and reload, etc. To know more about the details, please refer to our related paper:)","tags":["loam_livox","LiDAR SLAM","3D reconstruction"],"title":"LOAM_Livox: A robust LiDAR Odometry and Mapping (LOAM) package for Livox-LiDAR","type":"project"},{"authors":["Jiarong Lin","Luqi Wang","Fei Gao","Shaojie Shen","Fu Zhang"],"categories":null,"content":"Flying through a narrow gap using neural network: an end-to-end planning and control approach Crossgap_IL_RL is the open-soured project of our IROS_2019 paper ‚ÄúFlying through a narrow gap using neural network: an end-to-end planning and control approach‚Äù (our preprint version on arxiv, our video on Youtube) , including some of the training codes, pretrain networks, and simulator (based on Airsim).\nIntroduction: Our project can be divided into two phases,the imitation learning and reinforcement learning. In the first phase, we train our end-to-end policy network by imitating from a tradition pipeline. In the second phase, we fine-tune our policy network using reinforcement learning to improve the network performance. The framework of our systems is shown as follows.\nFine-tuning network using reinforcement-learning:\nOur realworld experiments:\n","date":1582848000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582848000,"objectID":"f0342c40730dd5bef46a0a7c1194da0c","permalink":"https://ziv-lin.github.io/publication/paper_cross_gap/","publishdate":"2020-02-28T00:00:00Z","relpermalink":"/publication/paper_cross_gap/","section":"publication","summary":"In this paper, we investigate the problem of enabling a drone to fly through a tilted narrow gap, without a traditional planning and control pipeline. To this end, we propose an end-to-end policy network, which imitates from the traditional pipeline and is fine-tuned using reinforcement learning. Unlike previous works which plan dynamical feasible trajectories using motion primitives and track the generated trajectory by a geometric controller, our proposed method is an end-to-end approach which takes the flight scenario as input and directly outputs thrust-attitude control commands for the quadrotor. Key contributions of our paper are: 1) presenting an imitate-reinforce training framework. 2) flying through a narrow gap using an end-to-end policy network, showing that learning based method can also address the highly dynamic control problem as the traditional pipeline does (see attached video https://www.youtube.com/watch?v=-HXARYlhat4  ). 3) propose a robust imitation of an optimal trajectory generator using multilayer perceptrons. 4) show how reinforcement learning can improve the performance of imitation learning, and the potential to achieve higher performance over the model-based method.","tags":["Reinforcement Learning","Motion Planning","UAV control"],"title":"Flying through a narrow gap using neural network: an end-to-end planning and control approach","type":"publication"},{"authors":["Wenliang Gao","Jiarong Lin","Fu Zhang","Shaojie Shen"],"categories":null,"content":"","date":1566432000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566432000,"objectID":"280f3841adbf4986e06511ce0e93b3d1","permalink":"https://ziv-lin.github.io/publication/paper_screen_base/","publishdate":"2019-08-22T00:00:00Z","relpermalink":"/publication/paper_screen_base/","section":"publication","summary":"For the manufacture of visual system product, it is necessary to calibrate a massive number of cameras in a limited time and space with a high consistency quality. Traditional calibration method with chessboard pattern is not suitable in the manufacturing industry since its requirement of motions leads to the problem of consistency, cost of space and time. In this work, we present a screen-based solution for automated camera intrinsic calibration on production lines. With screens clearly and easily displaying pixel points, the whole calibration pattern is formed with the dense and uniform points captured by the camera. The calibration accuracy is comparable with the traditional method with chessboard pattern. Unlike a variety of existing methods, our method needs little human interaction, as well as only a limited amount of space, making it easy to be deployed and operated in the industrial environments. With some experiments, we show the comparable performance of the system for perspective cameras and its potential in fisheye cameras with the developments of screens.","tags":["Calibration"],"title":"A Screen-Based Method for Automated Camera Intrinsic Calibration on Production Lines","type":"publication"},{"authors":["Wei Xu","Haowei Gu","Youming Qing","Jiarong Lin","Fu Zhang"],"categories":null,"content":"","date":1560211200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560211200,"objectID":"66a1c64654fee4d24f3d7828f47d793f","permalink":"https://ziv-lin.github.io/publication/paper_vtol/","publishdate":"2019-06-11T00:00:00Z","relpermalink":"/publication/paper_vtol/","section":"publication","summary":"In this paper, we present a full attitude control of an efficient quadrotor tail-sitter VTOL UAV with flexible modes. This control system is working in all flight modes without any control surfaces but motor differential thrusts. This paper concentrates on the design of the attitude controller and the altitude controller. For the attitude control, the controller's parameters and filters are optimized based on the frequency response model which is identified from the sweep experiment. As a result, the effect of system flexible modes is easily compensated in frequency-domain by using a notch filter, and the resulting attitude loop shows superior tracking performance and robustness. In the coordinated flight condition, the altitude controller is structured as the feedforward-feedback parallel controller. The feedforward thrust command is calculated based on the current speed and the pitch angle. Tests in hovering, forward accelerating and forward decelerating flights have been conducted to verify the proposed control system.","tags":["UAV control","VTOL"],"title":"Full attitude control of an efficient quadrotor tail-sitter VTOL UAV with flexible modes","type":"publication"},{"authors":["Jiarong Lin","Fu Zhang"],"categories":null,"content":"","date":1549929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549929600,"objectID":"66ec844e4bedec88412da7beff8a603e","permalink":"https://ziv-lin.github.io/publication/paper_loop_closure/","publishdate":"2019-02-12T00:00:00Z","relpermalink":"/publication/paper_loop_closure/","section":"publication","summary":"This paper presents a loop closure method to correct the long-term drift in LiDAR odometry and mapping (LOAM). Our proposed method computes the 2D histogram of keyframes, a local map patch, and uses the normalized cross-correlation of the 2D histograms as the similarity metric between the current keyframe and those in the map. We show that this method is fast, invariant to rotation, and produces reliable and accurate loop detection. The proposed method is implemented with careful engineering and integrated into the LOAM algorithm, forming a complete and practical system ready to use. To benefit the community by serving a benchmark for loop closure, the entire system is made open source on Github.","tags":["Loop detection","Loop closure","LiDAR SLAM"],"title":"A fast, complete, point cloud based loop closure for LiDAR odometry and mapping","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://ziv-lin.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]